{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNeAZxO9h6W5U2/gU3zuf4y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SohamNehra/Shakespeare-GPT/blob/master/Shakespeare_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FInH5xHoh4Sa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n"
      ],
      "metadata": {
        "id": "Uwi_HUyad-vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.manual_seed(1337)\n",
        "with open('./sample_data/input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d70_mHEveCVW",
        "outputId": "0c1fbeb7-3a9b-4baa-ee63-ba391c3ae34e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
      ],
      "metadata": {
        "id": "aUAuBakSeFnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "s3pBcFgAeKS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        #save the model\n",
        "        # torch.save(model.state_dict(), 'shakespeare_model.pt')\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "zUcV_MAYeOhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUJboLxLeU_K",
        "outputId": "53073e55-abed-41c3-d70e-fef1eb7e5111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.2849, val loss 4.2823\n",
            "step 100: train loss 2.4737, val loss 2.4894\n",
            "step 200: train loss 2.4166, val loss 2.4449\n",
            "step 300: train loss 2.3243, val loss 2.3535\n",
            "step 400: train loss 2.1415, val loss 2.1922\n",
            "step 500: train loss 2.0014, val loss 2.0889\n",
            "step 600: train loss 1.8856, val loss 2.0050\n",
            "step 700: train loss 1.7885, val loss 1.9325\n",
            "step 800: train loss 1.7125, val loss 1.8648\n",
            "step 900: train loss 1.6496, val loss 1.8185\n",
            "step 1000: train loss 1.6009, val loss 1.7823\n",
            "step 1100: train loss 1.5601, val loss 1.7425\n",
            "step 1200: train loss 1.5205, val loss 1.7134\n",
            "step 1300: train loss 1.4902, val loss 1.6866\n",
            "step 1400: train loss 1.4617, val loss 1.6604\n",
            "step 1500: train loss 1.4355, val loss 1.6420\n",
            "step 1600: train loss 1.4193, val loss 1.6291\n",
            "step 1700: train loss 1.3949, val loss 1.6064\n",
            "step 1800: train loss 1.3762, val loss 1.5990\n",
            "step 1900: train loss 1.3586, val loss 1.5862\n",
            "step 2000: train loss 1.3464, val loss 1.5750\n",
            "step 2100: train loss 1.3262, val loss 1.5569\n",
            "step 2200: train loss 1.3192, val loss 1.5573\n",
            "step 2300: train loss 1.3023, val loss 1.5403\n",
            "step 2400: train loss 1.2898, val loss 1.5282\n",
            "step 2500: train loss 1.2778, val loss 1.5290\n",
            "step 2600: train loss 1.2677, val loss 1.5238\n",
            "step 2700: train loss 1.2563, val loss 1.5227\n",
            "step 2800: train loss 1.2495, val loss 1.5156\n",
            "step 2900: train loss 1.2402, val loss 1.5067\n",
            "step 3000: train loss 1.2289, val loss 1.5020\n",
            "step 3100: train loss 1.2147, val loss 1.4980\n",
            "step 3200: train loss 1.2098, val loss 1.5005\n",
            "step 3300: train loss 1.2022, val loss 1.4973\n",
            "step 3400: train loss 1.1898, val loss 1.4899\n",
            "step 3500: train loss 1.1824, val loss 1.4885\n",
            "step 3600: train loss 1.1739, val loss 1.4897\n",
            "step 3700: train loss 1.1687, val loss 1.4864\n",
            "step 3800: train loss 1.1622, val loss 1.4862\n",
            "step 3900: train loss 1.1550, val loss 1.4845\n",
            "step 4000: train loss 1.1477, val loss 1.4731\n",
            "step 4100: train loss 1.1376, val loss 1.4873\n",
            "step 4200: train loss 1.1316, val loss 1.4856\n",
            "step 4300: train loss 1.1223, val loss 1.4773\n",
            "step 4400: train loss 1.1140, val loss 1.4747\n",
            "step 4500: train loss 1.1114, val loss 1.4809\n",
            "step 4600: train loss 1.1018, val loss 1.4737\n",
            "step 4700: train loss 1.0996, val loss 1.4800\n",
            "step 4800: train loss 1.0885, val loss 1.4769\n",
            "step 4900: train loss 1.0820, val loss 1.4728\n",
            "step 4999: train loss 1.0723, val loss 1.4800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=8000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AySzYmvkUzaa",
        "outputId": "6296a98d-7562-4e64-ceba-ccdf4f3b37cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "But with prison: I will steed for those\n",
            "meet wounds against the fury hours of Runer.\n",
            "\n",
            "Third Citizen:\n",
            "\n",
            "COMINIUS:\n",
            "Alrue, Marcius, &C this about of cupbol\n",
            "That beg the Capulet's harm, and with lessorr'd:\n",
            "All comparts an enrichly serviced that lie.\n",
            "\n",
            "First Servingman:\n",
            "How! what now this?\n",
            "\n",
            "MARIANA:\n",
            "Better, to it.\n",
            "\n",
            "First GREEN:\n",
            "Ay my lord, our fingers.\n",
            "\n",
            "ISABELLA:\n",
            "As I would fellow you go to sup,\n",
            "That wear Claudio were here: lethen\n",
            "About unstranch'd your holdiness richery,\n",
            "Infraitify the profins\n",
            "Of functions.\n",
            "\n",
            "AUTOLYCUS:\n",
            "Ere you hear go to Clarence,\n",
            "And you yet here hear me but his curfeit is fork.\n",
            "\n",
            "ESCALUS:\n",
            "He gift! be a unhappy thrifty bid this first,\n",
            "Save become by him my walls.\n",
            "\n",
            "LEONTES:\n",
            "Ever.\n",
            "\n",
            "LEONTES:\n",
            "Poor wife.\n",
            "\n",
            "Second Perdart:\n",
            "You would go what you might meas for your life at outwn.\n",
            "\n",
            "POLIXENES:\n",
            "You shall hath been force far?\n",
            "\n",
            "Chown:\n",
            "Where, that I had but thought which thou they say\n",
            "You pleaseme in stay on the vauch courage of Bohemond?\n",
            "\n",
            "POLIXENES:\n",
            "Never did!\n",
            "\n",
            "HERBELLIS:\n",
            "There will not be no more!\n",
            "Better her than I turn the brother's heart\n",
            "Is altender that he loves me will make,\n",
            "I fashiot sough for soldier which shore these wearisoes:\n",
            "My true are the limps of my loyal part;\n",
            "But that but brings was this hasteless, babes wherewered,\n",
            "And here all the temples there he carries\n",
            "At which Jean makes will leads.\n",
            "\n",
            "First Kue, Are you are my common unto\n",
            "To longer Jume than good my liege, but dhere\n",
            "Is maidenhead to the deep: what I have like.\n",
            "\n",
            "WARWICK:\n",
            "Soldom for all him to visatory,\n",
            "And give us see to, fortune slew to chaive me letter.\n",
            "\n",
            "CLIFFORD:\n",
            "Why, I so fear, but see, they say, let's tale the prince.\n",
            "\n",
            "RIVERS:\n",
            "Come forthwith Richard: these wall, go to harm.\n",
            "Thy father shall hearing, I fear: them are\n",
            "My noble house old Rome.\n",
            "\n",
            "CLAUDIO:\n",
            "I heardear, so die thee.\n",
            "\n",
            "ISABELLA:\n",
            "Being that I have wonder'd thyself and extress\n",
            "Because thee Dauncause of your Clarence,\n",
            "\n",
            "CLAUDIO:\n",
            "Who dares thou hast offers my own acching?\n",
            "\n",
            "Lord Germio, Prince Margaret, applaint,\n",
            "Come that we beaten, with us down grander.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "My lord, how a cholicy pur of death,\n",
            "And it mighty souls unto all our deserty\n",
            "Hot make powder to you; receive.\n",
            "\n",
            "LUCIO:\n",
            "Why, that he hath one was, and whether he less that\n",
            "ever this hand wherewither phurt; and do I have had\n",
            "By hope thy mother's reddical-he and cafe\n",
            "Cries: I will met that he shall bear him.\n",
            "\n",
            "VOLUMNIA:\n",
            "A man that this shall be this present know I do.\n",
            "Well answered the duke stroke and 'moieth.'\n",
            "\n",
            "MENENIUS:\n",
            "\n",
            "COMINIUS:\n",
            "Well, I pray you, my areween is made.\n",
            "\n",
            "CORIOLANUS:\n",
            "The prince deliver for Belingbroke: I turn shore\n",
            "Shall have weeds of Brancol-snowly. Brother,\n",
            "Your brave to pice defend ale entreaty;\n",
            "Let the covery Rome, down with Tumbling may meet,\n",
            "Could not say firm the authority of mine.\n",
            "\n",
            "ANTIGONUS:\n",
            "How, Warwick, what palens the sentlen with diath?\n",
            "\n",
            "CLARENCE:\n",
            "Who is all done?\n",
            "\n",
            "CAMILLO:\n",
            "Have you sorry here lady? then?\n",
            "Becale you Barnard peace by your highness?\n",
            "\n",
            "SLY:\n",
            "What by him wear mercy?\n",
            "\n",
            "LUCIO:\n",
            "Being she'll dead my socreful long.\n",
            "O mine, thy servant pain, get die in,\n",
            "I will and the certain it, but thy native man.\n",
            "Why 'tis my heart?\n",
            "\n",
            "Second men:\n",
            "One that thou fearing of a chos friend,\n",
            "'Tis this voice doth mish. Thou into the mods\n",
            "Of four blood friends, till F Cruncort of breath:\n",
            "Let thee thou how to crown my feest to the Lord Frorth\n",
            "Fettlingthrus mock more than what they had:\n",
            "I doubt by them again are I abunded.\n",
            "\n",
            "DUKE OF YORK:\n",
            "I will in Banished to't, be bade England\n",
            "By drunker, quoth and party Soul this defect:\n",
            "Tell him when I victory that this fain\n",
            "Engarance prospherous with Lancis: this lies and\n",
            "The King Leonta obedine's death; ran thou\n",
            "Seeking our twenty officery. Like up his offes,\n",
            "how art a treatury thousand prounder,\n",
            "I would cheek this haccies to draw up,\n",
            "And soon our fifty here knows my tender abuised\n",
            "Against Pudis.\n",
            "\n",
            "GLOUCESTER:\n",
            "My lord, that Henry hear and die\n",
            "That those any often-power I am sently hither:\n",
            "How hastenwious arm I elseBY?\n",
            "Or lean sorrow, I hold in my higheart!\n",
            "Whence I was your willsh.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Help me not, but I must I have here come in.\n",
            "\n",
            "LADY GREY:\n",
            "Take your highness!\n",
            "\n",
            "GLOUCESTER:\n",
            "Transport his proud friends: how doubly\n",
            "Dull not this daughter but full of the childh isle,\n",
            "That he well sall adventure.\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Therefore all beats true.\n",
            "\n",
            "PRINCE:\n",
            "Beat Juno; I'll know ask her: that thou know'st,\n",
            "As thou would do thy oatm to her, die.\n",
            "\n",
            "First Musician:\n",
            "What same?\n",
            "\n",
            "GREMIO:\n",
            "Do not what?\n",
            "\n",
            "DUKING RICHARD III:\n",
            "My lord, we master.\n",
            "\n",
            "Messenger:\n",
            "What! an art thou lest The cheeking tomb?\n",
            "\n",
            "CLARENCE:\n",
            "Ay, lord, who criest. Come on, go you are\n",
            "Is voice the city.\n",
            "\n",
            "KING RICHARD III:\n",
            "But what canst though\n",
            "No more to that? What meant with black feas it seems?\n",
            "Ah, sir, what's here? well she? night I cullently?\n",
            "Is what yield down you till you?\n",
            "\n",
            "BUSHOF MORIOLAND:\n",
            "Not line enough wenchaster, that\n",
            "Is your your more; he that fits, the name\n",
            "Cand thou art your friends takes;\n",
            "Lest numble it that he hard he works\n",
            "That alas half discordetion. But she comes\n",
            "Against thee the leaguement voul,\n",
            "And you will throw much death them not one,\n",
            "Which would not shed civil, unhonour my humour.\n",
            "See he, well hast thou draw'st me so! Thou\n",
            "Hast unto thy shearer's much than what he be so?\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "My liege, and my heart, so see your lord.\n",
            "\n",
            "GRY:\n",
            "\n",
            "Hear me, in good man, my true, sweet what you you bred\n",
            "Him love to. Would you have got to give it owe have?\n",
            "\n",
            "SIR PETHEN:\n",
            "Good justice, but come too my trade;\n",
            "The physiciply, you have done, you do hell,\n",
            "Your self, it up not couler wrong: alone.\n",
            "\n",
            "HENRY PEY:\n",
            "Are you hear histhought\n",
            "Better wit him his: in, let the skinsman go\n",
            "And that I can relieve our waics again.\n",
            "\n",
            "KING HENRY VI:\n",
            "It is a bull dull taste, and one womb,\n",
            "Who have from being over sure rouse of men at earth.\n",
            "Doth the depth in your ear:\n",
            "I have you better forgives your dance,\n",
            "But your guests in the humaAile of my potent.\n",
            "\n",
            "LADY GREY:\n",
            "Not Warwick, I would so, wot beseem your father's,\n",
            "I had been loved as occupation, you may take\n",
            "Of my liegent; I say your love enjoy.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "What, wilt thou draw not? Friend so your in grief,\n",
            "Or to size me gyving pate him fear'd for out,\n",
            "Were he seed of Horten' and and tide me when you show I,\n",
            "What I would step your willinguish majesty?\n",
            "\n",
            "RIVERS:\n",
            "Nay, I am so god thou a flower, no signify.\n",
            "\n",
            "GLOUCESTER:\n",
            "The little more horse eyes the match of my uthoum,\n",
            "Renown but my body time;\n",
            "And some painted. What, hate'er you comfort hither?\n",
            "\n",
            "MONES:\n",
            "Your swords curren commonishing obedience, you have\n",
            "The sechoy'd thanks, throught me dream on mysence:\n",
            "My other of it-such affected heart hath won\n",
            "What I fallen implicuted. I rather\n",
            "Lord Come on, and hereaf my amen.\n",
            "As milded am I satily order a, hath make?\n",
            "Why longing have you commune's hangether less? leteach\n",
            "A diaren so appel a Devori, and Derum\n",
            "Woe alwell upon your life, steeping in\n",
            "The law the jeast and beltogethery. Down,\n",
            "Where noble Lord , this death is fruit a tempt\n",
            "In the right. How they hath they did harm\n",
            "Upon their course from ten whole have prooted\n",
            "Countempts:'d heaven your fates for Ireland,\n",
            "cry, 'fain again hours. I pray my babe!\n",
            "There's not unextly brothers in your minage.\n",
            "I am well as far for vow\n",
            "Contraitors, tell on thee, therefore let that see him be\n",
            "Than this touching Aufidius. Do that take her\n",
            "loyalty; 'tis that my barbling way soul,\n",
            "Your neckle but as years upon your enemy.\n",
            "\n",
            "DUKE OF YORK:\n",
            "I have it lived, for our buring eye best,\n",
            "As to honesty in your flesh in his aid;\n",
            "Whe stouts as often like as that you lose.\n",
            "Will, and the remains that you have Tranio does thee,\n",
            "And therefore be decremended to furge sleep?\n",
            "\n",
            "ISABELLA:\n",
            "Thy sovereign nor harm my love proceed unto a queen\n",
            "And that I was not not so oft thy discharge?\n",
            "\n",
            "LADY ANNE:\n",
            "My lord of dearly, betiling thee god.\n",
            "Here at the gods most lives my faults: I stand,\n",
            "Their suddenly our gardens at the prince,\n",
            "Unbonder and any arms of your queen.\n",
            "\n",
            "VALERINA:\n",
            "Who promost so unwisely years?\n",
            "What before you Hannet, my wife heaven\n",
            "Ty brave handly thanks, that he will worm.\n",
            "These Iss weep they yet guide, upon the side,\n",
            "In thei\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"ShakespeareGPT.pth\")"
      ],
      "metadata": {
        "id": "NOt4-uMUzJ9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wIP-LZOoIpZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}